{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the jupyter notebook up down. Make sure all of the imported libraries in the first cell\n",
    "# have been installed\n",
    "# Note: Some code has been commented out if it became unused or was for debugging.\n",
    "# Some code can be uncommented (like the model loading code, for more details, read the comments in project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('Reviews.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df[\"Score\"].tolist()\n",
    "text = df[\"Text\"].tolist()\n",
    "\n",
    "def shuffle_two_lists(a, b):\n",
    "    '''\n",
    "    Shuffle two lists such that a[i] and b[i] are still paired together\n",
    "    a: list a\n",
    "    b: list b\n",
    "    '''\n",
    "    c = list(zip(a, b))\n",
    "    random.shuffle(c)\n",
    "    a, b = zip(*c)\n",
    "    return list(a), list(b)\n",
    "    \n",
    "# reference: https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "def remove_html_tags(string):\n",
    "    html_regex = re.compile('<.*?>')\n",
    "    cleantext = re.sub(html_regex, '', string)\n",
    "    return cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform basic preprocessing of data that is needed for both vader and Bert evaluations.\n",
    "# Note: Since Vader sentiment takes in sentences and provides a sentiment score, \n",
    "#       little text preprocessing will actually be common between Bert and vader.\n",
    "text = list(map(lambda sentence: remove_html_tags(sentence), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets.\n",
    "# Note, we didn't need to train the vader model because it is a rule based model. It does not change based\n",
    "# on the text.\n",
    "random.seed(42)\n",
    "scores, text = shuffle_two_lists(scores, text)\n",
    "X_train, X_test, y_train, y_test= train_test_split(text, scores, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take every sentence in X_test and evaluate it on vader sentiment.\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_prediction(model, sentence):\n",
    "    '''\n",
    "    Return a score between 1 - 5 based on the vader model's sentiment analysis classification\n",
    "    model: a vader sentiment intensity analyzer\n",
    "    sentence: a string that represents a review\n",
    "    '''\n",
    "    sentiment_dict = model.polarity_scores(sentence)\n",
    "    score = sentiment_dict['compound']\n",
    "    if score <= 0.2:\n",
    "        return 1\n",
    "    if score <= 0.4:\n",
    "        return 2\n",
    "    if score <= 0.6:\n",
    "        return 3\n",
    "    if score <= 0.8:\n",
    "        return 4\n",
    "    if score <= 1.0:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_predictions = list(map(lambda sentence: vader_prediction(vader, sentence), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1    0.36620   0.54521   0.43813     10572\n",
      "           2    0.09695   0.07716   0.08593      5923\n",
      "           3    0.10158   0.09667   0.09907      8555\n",
      "           4    0.15806   0.17230   0.16488     16024\n",
      "           5    0.75225   0.70057   0.72549     72617\n",
      "\n",
      "    accuracy                        0.53374    113691\n",
      "   macro avg    0.29501   0.31838   0.30270    113691\n",
      "weighted avg    0.54950   0.53374   0.53930    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print calculated recall, precision, and F-1 metrics for the vader model\n",
    "print(metrics.classification_report(y_test, vader_predictions, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now onto BERT. Most of the following code was modified from:\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert#define_your_model\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1\"\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    # handle preprocessingn needed for BERT\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    # Run data through the following BERT encoder\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    # Dropout layer (not sure if this is necessary, it was in the link above and the textbook mentions\n",
    "    # dropout layers being useful for NN performance so it was left, might change this later)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    # Add a dense output layer that uses softmax as its activation\n",
    "    net = tf.keras.layers.Dense(num_classes, activation=tf.keras.activations.softmax, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "# CategoricalCrossEntropy requires one-hot encoding of gold labels so we will need to do that when we create a \n",
    "# generator\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Run weighted F1 score as test metric for optimization\n",
    "model_metrics = tfa.metrics.F1Score(num_classes=num_classes, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug code\n",
    "# text_test = ['this is such an amazing movie!']\n",
    "# classifier_model = build_classifier_model(num_classes)\n",
    "# bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "# print(bert_raw_result)\n",
    "# print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = 2000\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.01*num_train_steps)\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model(num_classes)\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, num_classes: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    '''\n",
    "    linked_data = list(zip(X,y))\n",
    "    while True:\n",
    "        sample = random.sample(linked_data, num_sequences_per_batch)\n",
    "        sample_X = np.array(list(map(lambda s: s[0], sample)))\n",
    "        # NOTE: The y-class gets shifted by -1 to use to_categorical, we need to +1 to all predicted values later\n",
    "        # to get the right class!\n",
    "        sample_y = np.array(list(map(lambda s: s[1] - 1, sample)))\n",
    "        yield (np.array(sample_X), to_categorical(sample_y, num_classes=num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 4064s 2s/step - loss: 0.8909 - f1_score: 0.6187\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 4375s 2s/step - loss: 0.7755 - f1_score: 0.6682\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 5817s 3s/step - loss: 0.7403 - f1_score: 0.6853\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 3957s 2s/step - loss: 0.7240 - f1_score: 0.6923\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 3966s 2s/step - loss: 0.7137 - f1_score: 0.6972\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "num_sequences_per_batch = 128\n",
    "train_generator = data_generator(X_train, y_train, num_sequences_per_batch, num_classes)\n",
    "history = classifier_model.fit(x=train_generator,\n",
    "                               epochs=epochs,\n",
    "                               steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# functions for save and loading models\n",
    "# Uncomment if needed.\n",
    "# save current model\n",
    "# tf.keras.models.save_model(classifier_model, 'saved_weights', save_format='tf', overwrite=True, include_optimizer=False)\n",
    "# load saved model\n",
    "# NOTE: my submission includes my saved_weights folder which contain the weights I obtained at the end of the training\n",
    "# session. If this code is uncommented, all references to the classifier model below should be renamed to loaded_model\n",
    "# or vice-versa\n",
    "# loaded_model = load_model('saved_weights')\n",
    "# loaded_model.compile(optimizer=optimizer,\n",
    "#                          loss=loss,\n",
    "#                          metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_predictions = classifier_model.predict(X_test)\n",
    "bert_predictions = np.argmax(bert_predictions, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1    0.63494   0.71746   0.67368     10572\n",
      "           2    0.37303   0.16343   0.22728      5923\n",
      "           3    0.40616   0.33139   0.36498      8555\n",
      "           4    0.47348   0.24008   0.31861     16024\n",
      "           5    0.81617   0.94461   0.87571     72617\n",
      "\n",
      "    accuracy                        0.73735    113691\n",
      "   macro avg    0.54075   0.47939   0.49205    113691\n",
      "weighted avg    0.69708   0.73735   0.70619    113691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print calculated recall, precision, and F-1 metrics for the bert model\n",
    "# Note, we need to add 1, because we shifted class names by 1 at train time\n",
    "print(metrics.classification_report(y_test, bert_predictions, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06857816d295f49859cc9b744cb5307f357aa28072ae1f3fa176dda7f6176408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
