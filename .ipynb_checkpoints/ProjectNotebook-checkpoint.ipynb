{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6049d46",
   "metadata": {},
   "source": [
    "Todos: \n",
    "- Encoding: DONE\n",
    "- Models: use a SOTA transformer model? T5\n",
    "- Search/Ranking: Not that difficult once the model can produce predictions (use softmax/logit scores as relevancy score, can naively search pdf for location of citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc31dda9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in c:\\users\\andre\\anaconda3\\lib\\site-packages (0.63.9)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.23.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.5.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (2.7.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.3.5)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.13.6)\n",
      "Requirement already satisfied: transformers>=4.6.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (4.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (1.21.6)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.1.96)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2.7.0)\n",
      "Requirement already satisfied: regex in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2022.10.31)\n",
      "Requirement already satisfied: streamlit in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (0.11.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: pathtools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (41.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.29)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.0.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (2022.12.7)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (10.0.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.70.14)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: watchdog in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (4.2.4)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.2.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2.0)\n",
      "Requirement already satisfied: validators>=0.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: toml in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (12.6.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2)\n",
      "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.5)\n",
      "Requirement already satisfied: semver in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (6.2)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (2.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.41.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.10.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (19.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (2022.7)\n",
      "Requirement already satisfied: backports.zoneinfo in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.2.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.15.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from PyPDF2) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e853939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from cleantext import clean\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dc45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs():\n",
    "    source_pdfs = [] # single array of abstract text for sources\n",
    "    cited_pdfs = defaultdict(lambda: defaultdict(lambda: [])) # source paper --> {relevant: [], nonrelevant: []}\n",
    "    initial_path = './data/'\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        source_paper_path = join(initial_path, paper)\n",
    "        pdfs = [join(source_paper_path, f) for f in listdir(source_paper_path) if isfile(join(source_paper_path, f))]\n",
    "        source_pdfs.append(pdfs[0])\n",
    "        source_paper_path += '/Cited/'\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            cited_paper_path = source_paper_path + rel\n",
    "            cited_pdf_paths = [join(cited_paper_path, f) for f in listdir(cited_paper_path) if isfile(join(cited_paper_path, f))]\n",
    "            cited_pdfs[paper][rel] = cited_pdf_paths\n",
    "    return source_pdfs, cited_pdfs\n",
    "\n",
    "source_pdf_paths, cited_pdf_paths = get_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942cb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_page_text(input_list, path):\n",
    "    reader = PdfReader(path)\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if re.search('abstract', text, flags=re.I):\n",
    "            input_list.append(text)\n",
    "            return\n",
    "    input_list.append('none')\n",
    "\n",
    "def get_abstract_page(source_pdf_paths, cited_pdf_paths):\n",
    "    source_abstracts = [] # single array of abstract text for sources\n",
    "    cited_abstracts = defaultdict(lambda: defaultdict(lambda: [])) # source paper --> {relevant: [], nonrelevant: []}\n",
    "    for ppath in source_pdf_paths:\n",
    "        add_page_text(source_abstracts, ppath)\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            page_text_array = []\n",
    "            for ppath in cited_pdf_paths[paper][rel]:\n",
    "                add_page_text(page_text_array, ppath)\n",
    "            cited_abstracts[paper][rel] = page_text_array\n",
    "    return source_abstracts, cited_abstracts\n",
    "        \n",
    "source_abstracts, cited_abstracts = get_abstract_page(source_pdf_paths, cited_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd44efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return clean(text,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=False,                  # replace all URLs with a special token\n",
    "        no_emails=False,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "        no_numbers=False,               # replace all numbers with a special token\n",
    "        no_digits=False,                # replace all digits with a special token\n",
    "        no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "        no_punct=False,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "\n",
    "def clean_source_cite_pair(source_list, cited_dict):\n",
    "    cleaned_source_list = list(map(clean_text, source_list))\n",
    "    cleaned_cited_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            cleaned_cited_dict[paper][rel] = list(map(clean_text, cited_dict[paper][rel]))\n",
    "    return cleaned_source_list, cleaned_cited_dict\n",
    "\n",
    "cleaned_source_abstracts, cleaned_cited_abstracts = clean_source_cite_pair(source_abstracts, cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616906d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_helper(text):\n",
    "    if text == 'none':\n",
    "        return text\n",
    "    else:\n",
    "        array = text.split('abstract')\n",
    "        if len(array[1]) > 512:\n",
    "            return array[1]\n",
    "        else:\n",
    "            return array[1][:512]\n",
    "\n",
    "def truncate_to_512(input_list, input_dict):\n",
    "    output_list = []\n",
    "    output_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    for text in input_list:\n",
    "        output_list.append(truncate_helper(text))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            truncated_text_list = []\n",
    "            for text in input_dict[paper][rel]:\n",
    "                truncated_text_list.append(truncate_helper(text))\n",
    "            output_dict[paper][rel] = truncated_text_list\n",
    "    return output_list, output_dict\n",
    "\n",
    "source_abstract_text, cited_abstract_text = truncate_to_512(cleaned_source_abstracts, cleaned_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0245055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test abstract text w/ T5 model and see if stsb mode works well as a relevance classifier\n",
    "# generate prediction input\n",
    "prediction_input = []\n",
    "prefix = 'stsb'\n",
    "sent1 = 'sentence 1:'\n",
    "sent2 = 'sentence 2:'\n",
    "pred_input_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "for abs_text in source_abstract_text:\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            batch = []\n",
    "            for text in cited_abstract_text[paper][rel]:\n",
    "                batch.append(f'{prefix} {sent1} {abs_text}. {sent2} {text}')\n",
    "            pred_input_dict[paper][rel] = batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2ba6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model_args = T5Args()\n",
    "model_args.model_max_length = 1024\n",
    "model = T5Model(\"t5\", \"t5-small\", args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236c4739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e5b056ecf489bbe1b63bd8f2bbf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17cd43ce61c4b4ea46ddd28d529c86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f43941d2c9949039a8e28a2ced47302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1d967a2a114e0c9fd0d69b474f82b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f33682982b34abda7385172484feb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9633c928cdb4fffa1559eebbdb5b019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ed00807fb2454c8e2821c6ba3d289e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252d8351853f45a193c852a97e4dcff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40241519dd3d42b68ce521097cbba5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b325826144898bafa5bbc3dd5cd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24a390e18284827864e9a2d99c020dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004429ba68054c97981bc6841a400313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b09c8383704275b26d22c959a6ac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706615342b794187925e723a5e70d4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions = defaultdict(lambda: defaultdict(lambda: []))\n",
    "# papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "# for paper in papers:\n",
    "#     relevance = ['Relevant', 'Less Relevant']\n",
    "#     for rel in relevance:\n",
    "#         preds = model.predict(pred_input_dict[paper][rel])\n",
    "#         predictions[paper][rel] = preds\n",
    "predictions = defaultdict(lambda: [])\n",
    "papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "for paper in papers:\n",
    "    preds = model.predict(pred_input_dict[paper]['Relevant'] + pred_input_dict[paper]['Less Relevant'])\n",
    "    predictions[paper] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d12b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb mode in T5 doesn't work well for distinguishing between abstracts\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n"
     ]
    }
   ],
   "source": [
    "print('stsb mode in T5 doesn\\'t work well for distinguishing between abstracts')\n",
    "print(predictions['paper1'])\n",
    "print(predictions['paper2'])\n",
    "print(predictions['paper3'])\n",
    "print(predictions['paper4'])\n",
    "print(predictions['paper5'])\n",
    "print(predictions['paper6'])\n",
    "print(predictions['paper7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f1bfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('english') + list(punctuation) + ['\\n']\n",
    "    returned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            returned_tokens.append(token)\n",
    "    return returned_tokens\n",
    "\n",
    "def tokenize_source_cite_pair(input_list, input_dict):\n",
    "    tokenized_list = list(map(tokenize_text, input_list))\n",
    "    tokenized_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tokenized_dict[paper][rel] = list(map(tokenize_text, input_dict[paper][rel]))\n",
    "    return tokenized_list, tokenized_dict\n",
    "\n",
    "tokenized_source_abstracts, tokenized_cited_abstracts = tokenize_source_cite_pair(cleaned_source_abstracts, cleaned_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12b3446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_set(input_list, input_dict):\n",
    "    vocab = set()\n",
    "    for li in input_list:\n",
    "        for text in li: \n",
    "            vocab.add(text)\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            for li in input_dict[paper][rel]:\n",
    "                for text in li: \n",
    "                    vocab.add(text)\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocab_set(tokenized_source_abstracts, tokenized_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de9a4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq(input_list, input_dict):\n",
    "    source_tf_list = []\n",
    "    for doc in input_list:\n",
    "        source_tf_list.append(Counter(doc))\n",
    "    cited_tf_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tf_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tf_list.append(Counter(doc))\n",
    "            cited_tf_dict[paper][rel] = tf_list\n",
    "    return source_tf_list, cited_tf_dict\n",
    "\n",
    "source_tf_abstracts, cited_tf_abstracts = get_term_freq(tokenized_source_abstracts, tokenized_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e1d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num = len(source_tf_abstracts)\n",
    "papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "for paper in papers:\n",
    "    relevance = ['Relevant', 'Less Relevant']\n",
    "    for rel in relevance:\n",
    "        doc_num += len(cited_tf_abstracts[paper][rel])\n",
    "\n",
    "def get_doc_freq(word, input_list, input_dict):\n",
    "    doc_freq = 0\n",
    "    for doc in input_list:\n",
    "        if doc[word] != 0:\n",
    "            doc_freq += 1\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                if doc[word] != 0:\n",
    "                    doc_freq += 1\n",
    "    return doc_freq\n",
    "\n",
    "def get_idf_freq(vocab, input_list, input_dict, doc_num):\n",
    "    # pass in input_list/dict of counters for term counts\n",
    "    idf_dict = {}\n",
    "    for word in vocab:\n",
    "        word_doc_freq = get_doc_freq(word, input_list, input_dict)\n",
    "        for doc in input_list:\n",
    "            idf_value = np.log2(doc_num/word_doc_freq)\n",
    "            idf_dict[word] = idf_value\n",
    "    return idf_dict\n",
    "\n",
    "idf_dict = get_idf_freq(vocab, source_tf_abstracts, cited_tf_abstracts, doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed28ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_dict(vocab, tf_scores, idf_dict):\n",
    "    output_dict = {}\n",
    "    for word in vocab:\n",
    "        output_dict[word] = tf_scores[word]*idf_dict[word]\n",
    "    return output_dict\n",
    "    \n",
    "def get_tfidf_list_dict(vocab, input_list, input_dict, idf_dict):\n",
    "    source_tfidf_list = []\n",
    "    for doc in input_list:\n",
    "        source_tfidf_list.append(get_tfidf_dict(vocab, doc, idf_dict))\n",
    "    cited_tfidf_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tfidf_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tfidf_list.append(get_tfidf_dict(vocab, doc, idf_dict))\n",
    "            cited_tfidf_dict[paper][rel] = tfidf_list\n",
    "    return source_tfidf_list, cited_tfidf_dict\n",
    "\n",
    "source_tfidf_abstracts, cited_tfidf_abstracts = get_tfidf_list_dict(vocab, source_tf_abstracts, cited_tf_abstracts, idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c64430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tfidf_dict_to_vector(input_dict, vocab):\n",
    "    tfidf_vector = []\n",
    "    for word in vocab:\n",
    "        tfidf_vector.append(input_dict[word])\n",
    "    return np.array(tfidf_vector)\n",
    "\n",
    "def get_tfidf_vectorized_documents(input_list, input_dict):\n",
    "    source_tfidf_vectors = []\n",
    "    for doc in input_list:\n",
    "        source_tfidf_vectors.append(convert_tfidf_dict_to_vector(doc, vocab))\n",
    "    cited_tfidf_vectors = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tfidf_vector_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tfidf_vector_list.append(convert_tfidf_dict_to_vector(doc, vocab))\n",
    "            cited_tfidf_vectors[paper][rel] = tfidf_vector_list\n",
    "    return source_tfidf_vectors, cited_tfidf_vectors\n",
    "\n",
    "source_tfidf_vectors, cited_tfidf_vectors = get_tfidf_vectorized_documents(source_tfidf_abstracts, cited_tfidf_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b97a493f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.norm(a)*np.norm(b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86108e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "06857816d295f49859cc9b744cb5307f357aa28072ae1f3fa176dda7f6176408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
