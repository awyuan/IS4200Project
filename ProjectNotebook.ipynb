{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c082456e",
   "metadata": {},
   "source": [
    "Todos: \n",
    "- Encoding: DONE\n",
    "- Models: use a SOTA transformer model? T5\n",
    "- Search/Ranking: Not that difficult once the model can produce predictions (use softmax/logit scores as relevancy score, can naively search pdf for location of citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7aee52a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in c:\\users\\andre\\anaconda3\\lib\\site-packages (0.63.9)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.23.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.5.4)\n",
      "Requirement already satisfied: datasets in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (2.7.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.3.5)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.13.6)\n",
      "Requirement already satisfied: transformers>=4.6.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (4.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (1.21.6)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (0.1.96)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2.7.0)\n",
      "Requirement already satisfied: regex in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (2022.10.31)\n",
      "Requirement already satisfied: streamlit in c:\\users\\andre\\anaconda3\\lib\\site-packages (from simpletransformers) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from simpletransformers) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from transformers>=4.6.0->simpletransformers) (0.11.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: pathtools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (41.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.29)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.0.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests->simpletransformers) (2022.12.7)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (10.0.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\andre\\anaconda3\\lib\\site-packages (from datasets->simpletransformers) (0.70.14)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from pandas->simpletransformers) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: watchdog in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (4.2.4)\n",
      "Requirement already satisfied: pympler>=0.9 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (6.2.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2.0)\n",
      "Requirement already satisfied: validators>=0.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: toml in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (12.6.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (4.2)\n",
      "Requirement already satisfied: blinker>=1.0.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (1.5)\n",
      "Requirement already satisfied: semver in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: tornado>=5.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from streamlit->simpletransformers) (6.2)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers) (0.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (2.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (1.41.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard->simpletransformers) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tensorboard->simpletransformers) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\andre\\anaconda3\\lib\\site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.10.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.0.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (19.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: tzdata in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (2022.7)\n",
      "Requirement already satisfied: backports.zoneinfo in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.2.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from werkzeug>=0.11.15->tensorboard->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.15.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages (from PyPDF2) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\andre\\appdata\\roaming\\python\\python37\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ywin32 (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\andre\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a251d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from cleantext import clean\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ded1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs():\n",
    "    source_pdfs = [] # single array of abstract text for sources\n",
    "    cited_pdfs = defaultdict(lambda: defaultdict(lambda: [])) # source paper --> {relevant: [], nonrelevant: []}\n",
    "    initial_path = './data/'\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        source_paper_path = join(initial_path, paper)\n",
    "        pdfs = [join(source_paper_path, f) for f in listdir(source_paper_path) if isfile(join(source_paper_path, f))]\n",
    "        source_pdfs.append(pdfs[0])\n",
    "        source_paper_path += '/Cited/'\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            cited_paper_path = source_paper_path + rel\n",
    "            cited_pdf_paths = [join(cited_paper_path, f) for f in listdir(cited_paper_path) if isfile(join(cited_paper_path, f))]\n",
    "            cited_pdfs[paper][rel] = cited_pdf_paths\n",
    "    return source_pdfs, cited_pdfs\n",
    "\n",
    "source_pdf_paths, cited_pdf_paths = get_pdfs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e27d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_page_text(input_list, path):\n",
    "    reader = PdfReader(path)\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if re.search('abstract', text, flags=re.I):\n",
    "            input_list.append(text)\n",
    "            return\n",
    "    input_list.append('none')\n",
    "\n",
    "def get_abstract_page(source_pdf_paths, cited_pdf_paths):\n",
    "    source_abstracts = [] # single array of abstract text for sources\n",
    "    cited_abstracts = defaultdict(lambda: defaultdict(lambda: [])) # source paper --> {relevant: [], nonrelevant: []}\n",
    "    for ppath in source_pdf_paths:\n",
    "        add_page_text(source_abstracts, ppath)\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            page_text_array = []\n",
    "            for ppath in cited_pdf_paths[paper][rel]:\n",
    "                add_page_text(page_text_array, ppath)\n",
    "            cited_abstracts[paper][rel] = page_text_array\n",
    "    return source_abstracts, cited_abstracts\n",
    "        \n",
    "source_abstracts, cited_abstracts = get_abstract_page(source_pdf_paths, cited_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743254e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return clean(text,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=False,                  # replace all URLs with a special token\n",
    "        no_emails=False,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "        no_numbers=False,               # replace all numbers with a special token\n",
    "        no_digits=False,                # replace all digits with a special token\n",
    "        no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "        no_punct=False,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "\n",
    "def clean_source_cite_pair(source_list, cited_dict):\n",
    "    cleaned_source_list = list(map(clean_text, source_list))\n",
    "    cleaned_cited_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            cleaned_cited_dict[paper][rel] = list(map(clean_text, cited_dict[paper][rel]))\n",
    "    return cleaned_source_list, cleaned_cited_dict\n",
    "\n",
    "cleaned_source_abstracts, cleaned_cited_abstracts = clean_source_cite_pair(source_abstracts, cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb588b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_helper(text):\n",
    "    if text == 'none':\n",
    "        return text\n",
    "    else:\n",
    "        array = text.split('abstract')\n",
    "        if len(array[1]) > 512:\n",
    "            return array[1]\n",
    "        else:\n",
    "            return array[1][:512]\n",
    "\n",
    "def truncate_to_512(input_list, input_dict):\n",
    "    output_list = []\n",
    "    output_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    for text in input_list:\n",
    "        output_list.append(truncate_helper(text))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            truncated_text_list = []\n",
    "            for text in input_dict[paper][rel]:\n",
    "                truncated_text_list.append(truncate_helper(text))\n",
    "            output_dict[paper][rel] = truncated_text_list\n",
    "    return output_list, output_dict\n",
    "\n",
    "source_abstract_text, cited_abstract_text = truncate_to_512(cleaned_source_abstracts, cleaned_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5b1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test abstract text w/ T5 model and see if stsb mode works well as a relevance classifier\n",
    "# generate prediction input\n",
    "prediction_input = []\n",
    "prefix = 'stsb'\n",
    "sent1 = 'sentence 1:'\n",
    "sent2 = 'sentence 2:'\n",
    "pred_input_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "for abs_text in source_abstract_text:\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            batch = []\n",
    "            for text in cited_abstract_text[paper][rel]:\n",
    "                batch.append(f'{prefix} {sent1} {abs_text}. {sent2} {text}')\n",
    "            pred_input_dict[paper][rel] = batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c80800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f0334e8f3440c0b6412e71ce1ad1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python37\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2249b7daaac425d8b08268ea07ed274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14fcbfabf15440e9192e40c49ba1a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model_args = T5Args()\n",
    "model_args.model_max_length = 1024\n",
    "model = T5Model(\"t5\", \"t5-small\", args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c503716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af1f3b8bb204a02991134543b9393b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils_base.py:3704: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee9c40aff764bed96797366dcfd915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5485983a05c44663992d098e3e99f9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b12ce5d41943298d81361ea24a290a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b770cbbf9b046adb3c40aa40f2a7cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e5dfb7059c4600aaa4f719408c2908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f229127925f48a1b94b65a5767be136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c2e0e43d81489b813b3f98843423e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30db5dc33c04dd3b0180d7499314a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03882415df3749f1af5ce9b2b881493a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837ab975bd9e450ca82c24dd2b0e5e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a3e70c93ab4cd1aa01f1243965414d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66de3b34a4d64ce98e7c4ebf37c364ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b9ea0d6dbf43e88acbf04865b2bab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions = defaultdict(lambda: defaultdict(lambda: []))\n",
    "# papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "# for paper in papers:\n",
    "#     relevance = ['Relevant', 'Less Relevant']\n",
    "#     for rel in relevance:\n",
    "#         preds = model.predict(pred_input_dict[paper][rel])\n",
    "#         predictions[paper][rel] = preds\n",
    "predictions = defaultdict(lambda: [])\n",
    "papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "for paper in papers:\n",
    "    preds = model.predict(pred_input_dict[paper]['Relevant'] + pred_input_dict[paper]['Less Relevant'])\n",
    "    predictions[paper] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4ba527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb mode in T5 doesn't work well for distinguishing between abstracts\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n",
      "['1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8', '1.8']\n"
     ]
    }
   ],
   "source": [
    "print('stsb mode in T5 doesn\\'t work well for distinguishing between abstracts')\n",
    "print(predictions['paper1'])\n",
    "print(predictions['paper2'])\n",
    "print(predictions['paper3'])\n",
    "print(predictions['paper4'])\n",
    "print(predictions['paper5'])\n",
    "print(predictions['paper6'])\n",
    "print(predictions['paper7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce4f3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = stopwords.words('english') + list(punctuation) + ['\\n']\n",
    "    returned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            returned_tokens.append(token)\n",
    "    return returned_tokens\n",
    "\n",
    "def tokenize_source_cite_pair(input_list, input_dict):\n",
    "    tokenized_list = list(map(tokenize_text, input_list))\n",
    "    tokenized_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tokenized_dict[paper][rel] = list(map(tokenize_text, input_dict[paper][rel]))\n",
    "    return tokenized_list, tokenized_dict\n",
    "\n",
    "tokenized_source_abstracts, tokenized_cited_abstracts = tokenize_source_cite_pair(cleaned_source_abstracts, cleaned_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40df621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_set(input_list, input_dict):\n",
    "    vocab = set()\n",
    "    for li in input_list:\n",
    "        for text in li: \n",
    "            vocab.add(text)\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            for li in input_dict[paper][rel]:\n",
    "                for text in li: \n",
    "                    vocab.add(text)\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocab_set(tokenized_source_abstracts, tokenized_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6443fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq(input_list, input_dict):\n",
    "    source_tf_list = []\n",
    "    for doc in input_list:\n",
    "        source_tf_list.append(Counter(doc))\n",
    "    cited_tf_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tf_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tf_list.append(Counter(doc))\n",
    "            cited_tf_dict[paper][rel] = tf_list\n",
    "    return source_tf_list, cited_tf_dict\n",
    "\n",
    "source_tf_abstracts, cited_tf_abstracts = get_term_freq(tokenized_source_abstracts, tokenized_cited_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11e29108",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num = len(source_tf_abstracts)\n",
    "papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "for paper in papers:\n",
    "    relevance = ['Relevant', 'Less Relevant']\n",
    "    for rel in relevance:\n",
    "        doc_num += len(cited_tf_abstracts[paper][rel])\n",
    "\n",
    "def get_doc_freq(word, input_list, input_dict):\n",
    "    doc_freq = 0\n",
    "    for doc in input_list:\n",
    "        if doc[word] != 0:\n",
    "            doc_freq += 1\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                if doc[word] != 0:\n",
    "                    doc_freq += 1\n",
    "    return doc_freq\n",
    "\n",
    "def get_idf_freq(vocab, input_list, input_dict, doc_num):\n",
    "    # pass in input_list/dict of counters for term counts\n",
    "    idf_dict = {}\n",
    "    for word in vocab:\n",
    "        word_doc_freq = get_doc_freq(word, input_list, input_dict)\n",
    "        for doc in input_list:\n",
    "            idf_value = np.log2(doc_num/word_doc_freq)\n",
    "            idf_dict[word] = idf_value\n",
    "    return idf_dict\n",
    "\n",
    "idf_dict = get_idf_freq(vocab, source_tf_abstracts, cited_tf_abstracts, doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a29b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_dict(vocab, tf_scores, idf_dict):\n",
    "    output_dict = {}\n",
    "    for word in vocab:\n",
    "        output_dict[word] = tf_scores[word]*idf_dict[word]\n",
    "    return output_dict\n",
    "    \n",
    "def get_tfidf_list_dict(vocab, input_list, input_dict, idf_dict):\n",
    "    source_tfidf_list = []\n",
    "    for doc in input_list:\n",
    "        source_tfidf_list.append(get_tfidf_dict(vocab, doc, idf_dict))\n",
    "    cited_tfidf_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tfidf_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tfidf_list.append(get_tfidf_dict(vocab, doc, idf_dict))\n",
    "            cited_tfidf_dict[paper][rel] = tfidf_list\n",
    "    return source_tfidf_list, cited_tfidf_dict\n",
    "\n",
    "source_tfidf_abstracts, cited_tfidf_abstracts = get_tfidf_list_dict(vocab, source_tf_abstracts, cited_tf_abstracts, idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82e22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tfidf_dict_to_vector(input_dict, vocab):\n",
    "    tfidf_vector = []\n",
    "    for word in vocab:\n",
    "        tfidf_vector.append(input_dict[word])\n",
    "    return np.array(tfidf_vector)\n",
    "\n",
    "def get_tfidf_vectorized_documents(input_list, input_dict):\n",
    "    source_tfidf_vectors = []\n",
    "    for doc in input_list:\n",
    "        source_tfidf_vectors.append(convert_tfidf_dict_to_vector(doc, vocab))\n",
    "    cited_tfidf_vectors = defaultdict(lambda: defaultdict(lambda: []))\n",
    "    papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "    for paper in papers:\n",
    "        relevance = ['Relevant', 'Less Relevant']\n",
    "        for rel in relevance:\n",
    "            tfidf_vector_list = []\n",
    "            for doc in input_dict[paper][rel]:\n",
    "                tfidf_vector_list.append(convert_tfidf_dict_to_vector(doc, vocab))\n",
    "            cited_tfidf_vectors[paper][rel] = tfidf_vector_list\n",
    "    return source_tfidf_vectors, cited_tfidf_vectors\n",
    "\n",
    "source_tfidf_vectors, cited_tfidf_vectors = get_tfidf_vectorized_documents(source_tfidf_abstracts, cited_tfidf_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6ab90e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "def get_most_relevant(source_document_vector, cited_document_vectors_relevant, cited_document_vectors_less_relevant):\n",
    "    sim_scores = []\n",
    "    for i, rel_doc in enumerate(cited_document_vectors_relevant):\n",
    "        sim_scores.append(('Relevant', i, cosine_sim(source_document_vector, rel_doc)))\n",
    "    for i, not_rel_doc in enumerate(cited_document_vectors_less_relevant):\n",
    "        sim_scores.append(('Less Relevant', i, cosine_sim(source_document_vector, not_rel_doc)))\n",
    "    return sorted(sim_scores, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e2dfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_index_to_cited_pdfs(index, paper, relevance, cited_pdf_paths):\n",
    "    return cited_pdf_paths[paper][relevance][index]\n",
    "\n",
    "def calculate_paper_relevance_scores(source_tfidf_vectors, cited_tfidf_vectors, source_pdf_paths, cited_pdf_paths):\n",
    "    output_dict = {}\n",
    "    for i, source_vector in enumerate(source_tfidf_vectors):\n",
    "        papers = ['paper1', 'paper2', 'paper3', 'paper4', 'paper5', 'paper6', 'paper7']\n",
    "        for paper in papers:\n",
    "            relevance_scores = get_most_relevant(source_vector, cited_tfidf_vectors[paper]['Relevant'], cited_tfidf_vectors[paper]['Less Relevant'])\n",
    "            output_dict[source_pdf_paths[i]] = list(map(lambda x: (x[0], map_index_to_cited_pdfs(x[1], paper, x[0], cited_pdf_paths), x[2]), relevance_scores))\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0391ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_dict = calculate_paper_relevance_scores(source_tfidf_vectors, cited_tfidf_vectors, source_pdf_paths, cited_pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb3472f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@4 for VSM model using TFIDF Vectors: 0.7380952380952381\n",
      "AP@4 for ./data/paper1\\Shafahi2018.pdf 0.9375\n",
      "AP@4 for ./data/paper2\\Jagieski2020.pdf 1.0\n",
      "AP@4 for ./data/paper3\\Biggio2017.pdf 0.41666666666666663\n",
      "AP@4 for ./data/paper4\\Brusseau2022.pdf 0.8541666666666666\n",
      "AP@4 for ./data/paper5\\Dobrev2022.pdf 1.0\n",
      "AP@4 for ./data/paper6\\Morsa2022.pdf 0.47916666666666663\n",
      "AP@4 for ./data/paper7\\Gohar2022.pdf 0.47916666666666663\n"
     ]
    }
   ],
   "source": [
    "def calculate_avg_prec(relevance_scores, num):\n",
    "    prec_num = 0\n",
    "    prec_denom = 0\n",
    "    cumulative = 0\n",
    "    for i in range(num):\n",
    "        prec_denom += 1\n",
    "        if relevance_scores[i][0] == 'Relevant':\n",
    "            prec_num += 1\n",
    "        cumulative += prec_num / prec_denom\n",
    "    return cumulative / num\n",
    "\n",
    "def calculate_map(relevance_dict, num):\n",
    "    precisions = []\n",
    "    for key in relevance_dict.keys():\n",
    "        precisions.append(calculate_avg_prec(relevance_dict[key], num))\n",
    "    return sum(precisions)/len(precisions)\n",
    "\n",
    "print('MAP@4 for VSM model using TFIDF Vectors:', calculate_map(relevance_dict, 4))\n",
    "# Individual AP@4\n",
    "for key in relevance_dict.keys():\n",
    "    print(f'AP@4 for {key}', calculate_avg_prec(relevance_dict[key], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f89da9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the top4 most relevant documents according to the VSM model\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper1\\Shafahi2018.pdf\n",
      "[('Relevant', './data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.03478826233833435), ('Relevant', './data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.027017218337367855), ('Relevant', './data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.016867298059795817), ('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.015772062713223494)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper2\\Jagieski2020.pdf\n",
      "[('Relevant', './data/paper7/Cited/Relevant\\\\Hougardy2010.pdf', 0.029245724444194106), ('Relevant', './data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.022833135084655336), ('Relevant', './data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.01914741069739826), ('Relevant', './data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.01913156061278095)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper3\\Biggio2017.pdf\n",
      "[('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.024665389345255286), ('Relevant', './data/paper7/Cited/Relevant\\\\OSullivan2019.pdf', 0.017964017615181002), ('Relevant', './data/paper7/Cited/Relevant\\\\Hougardy2010.pdf', 0.0156666318378507), ('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Gonzalez2020.pdf', 0.014854097213165137)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper4\\Brusseau2022.pdf\n",
      "[('Relevant', './data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.0653214452188587), ('Relevant', './data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.025080635272895755), ('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.01623925582697717), ('Relevant', './data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.014385126107518707)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper5\\Dobrev2022.pdf\n",
      "[('Relevant', './data/paper7/Cited/Relevant\\\\Katanforoosh2019.pdf', 1.0), ('Relevant', './data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.0), ('Relevant', './data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.0), ('Relevant', './data/paper7/Cited/Relevant\\\\Cheng2011.pdf', 0.0)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper6\\Morsa2022.pdf\n",
      "[('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.03007727059583325), ('Relevant', './data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.021331350158864053), ('Relevant', './data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.021108789723722692), ('Relevant', './data/paper7/Cited/Relevant\\\\Fajjari2011.pdf', 0.0175145853409671)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper7\\Gohar2022.pdf\n",
      "[('Less Relevant', './data/paper7/Cited/Less Relevant\\\\Gonzalez2020.pdf', 0.3902820394887455), ('Relevant', './data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.2877580681791038), ('Relevant', './data/paper7/Cited/Relevant\\\\Cheng2011.pdf', 0.2253997995183469), ('Relevant', './data/paper7/Cited/Relevant\\\\Fajjari2011.pdf', 0.22117651420669845)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below are the top4 most relevant documents according to the VSM model')\n",
    "for key in relevance_dict.keys():\n",
    "    print('')\n",
    "    print(f'Top 4 most relevant documents for {key}')\n",
    "    print(relevance_dict[key][:4])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "930eff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the top4 most relevant documents according to the VSM model\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper1\\Shafahi2018.pdf\n",
      "[('./data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.03478826233833435), ('./data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.027017218337367855), ('./data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.016867298059795817), ('./data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.015772062713223494)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper2\\Jagieski2020.pdf\n",
      "[('./data/paper7/Cited/Relevant\\\\Hougardy2010.pdf', 0.029245724444194106), ('./data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.022833135084655336), ('./data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.01914741069739826), ('./data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.01913156061278095)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper3\\Biggio2017.pdf\n",
      "[('./data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.024665389345255286), ('./data/paper7/Cited/Relevant\\\\OSullivan2019.pdf', 0.017964017615181002), ('./data/paper7/Cited/Relevant\\\\Hougardy2010.pdf', 0.0156666318378507), ('./data/paper7/Cited/Less Relevant\\\\Gonzalez2020.pdf', 0.014854097213165137)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper4\\Brusseau2022.pdf\n",
      "[('./data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.0653214452188587), ('./data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.025080635272895755), ('./data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.01623925582697717), ('./data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.014385126107518707)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper5\\Dobrev2022.pdf\n",
      "[('./data/paper7/Cited/Relevant\\\\Katanforoosh2019.pdf', 1.0), ('./data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.0), ('./data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.0), ('./data/paper7/Cited/Relevant\\\\Cheng2011.pdf', 0.0)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper6\\Morsa2022.pdf\n",
      "[('./data/paper7/Cited/Less Relevant\\\\Olimid12020.pdf', 0.03007727059583325), ('./data/paper7/Cited/Relevant\\\\Berner2019.pdf', 0.021331350158864053), ('./data/paper7/Cited/Relevant\\\\Baker2020.pdf', 0.021108789723722692), ('./data/paper7/Cited/Relevant\\\\Fajjari2011.pdf', 0.0175145853409671)]\n",
      "\n",
      "\n",
      "Top 4 most relevant documents for ./data/paper7\\Gohar2022.pdf\n",
      "[('./data/paper7/Cited/Less Relevant\\\\Gonzalez2020.pdf', 0.3902820394887455), ('./data/paper7/Cited/Relevant\\\\Gong2014.pdf', 0.2877580681791038), ('./data/paper7/Cited/Relevant\\\\Cheng2011.pdf', 0.2253997995183469), ('./data/paper7/Cited/Relevant\\\\Fajjari2011.pdf', 0.22117651420669845)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below are the top4 most relevant documents according to the VSM model')\n",
    "for key in relevance_dict.keys():\n",
    "    print('')\n",
    "    print(f'Top 4 most relevant documents for {key}')\n",
    "    print(list(map(lambda x: (x[1], x[2]), relevance_dict[key][:4])))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93dab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06055011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "06857816d295f49859cc9b744cb5307f357aa28072ae1f3fa176dda7f6176408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
